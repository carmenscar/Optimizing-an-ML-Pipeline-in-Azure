# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
This project aims to compare the results of using **HyperDrive** and **AutoML** in Azure Machine Learning. The dataset contains data about bank marketing campaigns, and the goal is to predict whether a client will subscribe to a term deposit (binary classification).

- **Best HyperDrive Model**: Logistic Regression with an accuracy of **90.88%**.
- **Best AutoML Model**: [Insert Model Name] with an accuracy of **91.67%**.

The results do not show a statistically significant difference between the two approaches.

## Scikit-learn Pipeline
### Pipeline Architecture:
1. **Data Loading**: The dataset is loaded using the `TabularDatasetFactory` from a URL.
2. **Data Cleaning**: The `clean_data` function is applied to handle missing values, encode categorical variables, and prepare the data for training.
3. **Hyperparameter Tuning**: HyperDrive is used to optimize the hyperparameters of the Logistic Regression model. The hyperparameters tuned are:
   - `C`: Inverse of regularization strength.
   - `max_iter`: Maximum number of iterations for convergence.
4. **Classification Algorithm**: A Logistic Regression model is trained using Scikit-learn.

### Parameter Sampler:
- **RandomParameterSampling**: This method randomly samples hyperparameters from a defined search space. It is efficient and allows for exploration of a wide range of values without being exhaustive.

### Early Stopping Policy:
- **BanditPolicy**: This policy stops runs that are not performing well based on a slack factor. It helps save computational resources by terminating underperforming runs early.

## AutoML
### Data Preparation:
1. **Data Cleaning**: The `clean_data` function is applied to clean and preprocess the dataset.
2. **Column Name Adjustment**: To ensure compatibility with the `TabularDatasetFactory` and AutoML, the names of the columns were adjusted by replacing dots (`.`) with underscores (`_`). This step was necessary to pass the dataset validation and allow the model to run successfully in AutoML.
3. **Dataset Registration**: The cleaned and adjusted dataset is saved using `Dataset.Tabular` and passed to AutoML for training.
### Model and Hyperparameters:
AutoML automatically tested several models, including Random Forest, Gradient Boosting, and Logistic Regression. The best model generated by AutoML was a **VotingEnsemble**, with hyperparameters optimized for accuracy.

## Pipeline comparison
- **Accuracy**: The Logistic Regression model optimized with HyperDrive achieved an accuracy of **90.88%**, while the AutoML model achieved an accuracy of **91.67%**.
- **Architecture**: The HyperDrive pipeline focused on tuning a single model (Logistic Regression), while AutoML explored multiple models and selected the best one.
- **Differences**: The difference in accuracy is minimal and does not appear to be statistically significant. This suggests that both approaches are equally effective for this specific problem.

## Future work
1. **Feature Engineering**: Explore additional feature engineering techniques to improve model performance.
2. **Hyperparameter Tuning**: Use a more exhaustive search method, such as Grid Search, for hyperparameter tuning.
4. **Increased Iterations and Hyperparameters**: In this project, the model was trained with a limited number of iterations and hyperparameter combinations to prioritize speed. However, it may be worthwhile to explore more iterations and hyperparameter combinations to potentially achieve better results.

## Proof of cluster clean up
I used more than one user to complete the course project. The last cluster related to the final stage of development was deleted, as shown in the attached image. However, it is necessary to verify if there are other clusters that were created earlier during the initial stages of development and need to be deleted.
![Cluster Deletion](deleting_cluster.png)
